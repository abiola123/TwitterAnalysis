{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"> LSIR LAB - SEMESTER PROJECT</h1> \n",
    "\n",
    "**Students:**\n",
    "- Etienne BRUNO\n",
    "- Abiola ADEYE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD SAMPLE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spark and open json file\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.read.json('../data/sample/sample.jsonl')\n",
    "json_rdd = df.rdd\n",
    "df.printSchema()\n",
    "spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENGLISH SAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tweets into an rdd\n",
    "json_rdd_en = json_rdd.filter(lambda x: x['includes'] and x['data'] and x['data']['lang'] == 'en')\n",
    "\n",
    "# Create a dataframe with the data we want for the English tweets\n",
    "# id, author_id, created_at, retweet_count, reply_count, like_count, quote_count, impression_count, followers_count\n",
    "# We need to use the index [0] because the includes field is a list of users and we only want the first one as the tweet is only associated with one user\n",
    "# the other users are the users mentioned in the tweet (if any)\n",
    "en_df = json_rdd_en.map(lambda x: [ x['data']['id'],\n",
    "                                    x['data']['author_id'],\n",
    "                                    x['data']['created_at'],\n",
    "                                    x['data']['public_metrics']['retweet_count'],\n",
    "                                    x['data']['public_metrics']['reply_count'],\n",
    "                                    x['data']['public_metrics']['like_count'],\n",
    "                                    x['data']['public_metrics']['quote_count'],\n",
    "                                    x['data']['public_metrics']['impression_count'],\n",
    "                                    x['includes']['users'][0]['public_metrics']['followers_count']]).toDF(['id', 'author_id', 'created_at', 'retweet_count', 'reply_count', 'like_count', 'quote_count', 'impression_count', 'followers_count'])\n",
    "en_df.show(3)\n",
    "\n",
    "# Save as parquet\n",
    "en_df.write.parquet('../data/sample/sample_en_parquet/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FRENCH SAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load French tweets into an rdd\n",
    "json_rdd_fr = json_rdd.filter(lambda x: x['includes'] and x['data'] and x['data']['lang'] == 'fr')\n",
    "\n",
    "# Create a dataframe with the data we want for the French tweets\n",
    "# id, author_id, created_at, retweet_count, reply_count, like_count, quote_count, impression_count, followers_count\n",
    "# We need to use the index [0] because the includes field is a list of users and we only want the first one as the tweet is only associated with one user\n",
    "# the other users are the users mentioned in the tweet (if any)\n",
    "fr_df = json_rdd_fr.map(lambda x: [ x['data']['id'],\n",
    "                                    x['data']['author_id'],\n",
    "                                    x['data']['created_at'],\n",
    "                                    x['data']['public_metrics']['retweet_count'],\n",
    "                                    x['data']['public_metrics']['reply_count'],\n",
    "                                    x['data']['public_metrics']['like_count'],\n",
    "                                    x['data']['public_metrics']['quote_count'],\n",
    "                                    x['data']['public_metrics']['impression_count'],\n",
    "                                    x['includes']['users'][0]['public_metrics']['followers_count']]).toDF(['id', 'author_id', 'created_at', 'retweet_count', 'reply_count', 'like_count', 'quote_count', 'impression_count', 'followers_count'])\n",
    "en_df.show(3)\n",
    "\n",
    "# Save as parquet\n",
    "fr_df.write.parquet('../data/sample/sample_fr_parquet/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD POLITICIANS DATA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HELPERS FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tweet_data(x):\n",
    "    return [\n",
    "        x['id'],\n",
    "        x['author_id'],\n",
    "        x['created_at'],\n",
    "        x['public_metrics']['retweet_count'],\n",
    "        x['public_metrics']['reply_count'],\n",
    "        x['public_metrics']['like_count'],\n",
    "        x['public_metrics']['quote_count'],\n",
    "        x['public_metrics']['impression_count'],\n",
    "    ]\n",
    "\n",
    "\n",
    "def extract_user_metadata(x):\n",
    "    return [\n",
    "        x['id'],\n",
    "        x['public_metrics']['followers_count'],\n",
    "        x['public_metrics']['following_count'],\n",
    "        x['public_metrics']['tweet_count'],\n",
    "        x['public_metrics']['listed_count'],\n",
    "    ]\n",
    "\n",
    "\n",
    "tweet_data_columns = ['id', 'author_id', 'created_at', 'retweet_count', 'reply_count', 'like_count', 'quote_count', 'impression_count']\n",
    "user_metadata_columns = ['author_id','followers_count', 'following_count', 'tweet_count', 'listed_count']\n",
    "\n",
    "\n",
    "def tweet_and_user_data(json_rdd):\n",
    "    json_rdd_tweet_data = json_rdd.filter(lambda x: x['data']) \\\n",
    "                        .flatMap(lambda x: x['data']) \\\n",
    "                        .map(extract_tweet_data) \\\n",
    "                        .toDF(tweet_data_columns)\n",
    "    \n",
    "    json_rdd_user_data = json_rdd.filter(lambda x: x['includes']) \\\n",
    "                        .map(lambda x: x['includes']['users'][0]) \\\n",
    "                        .map(extract_user_metadata) \\\n",
    "                        .toDF(user_metadata_columns)\n",
    "    \n",
    "    return json_rdd_tweet_data.join(json_rdd_user_data, on='author_id', how='left')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### US POLITICIANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.read.json('../data/american_politicians/tweets.jsonl')\n",
    "json_rdd = df.rdd\n",
    "#df.printSchema()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tweet and user data only from the rdd\n",
    "us_politicians_df = tweet_and_user_data(json_rdd)\n",
    "us_politicians_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as parquet\n",
    "us_politicians_df.write.parquet('../data/american_politicians/parquet/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FRENCH POLITICIANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load French politicians tweets into an rdd\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.read.json('../data/french_politicians/tweets.jsonl')\n",
    "json_rdd = df.rdd\n",
    "#df.printSchema()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tweet and user data only from the rdd\n",
    "fr_politicians_df = tweet_and_user_data(json_rdd)\n",
    "fr_politicians_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as parquet\n",
    "fr_politicians_df.write.parquet('../data/french_politicians/parquet/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD CELEBRITIES DATA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### US CELEBRITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load US celebrities tweets into an rdd\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.read.json('../data/american_celebrities/tweets.jsonl')\n",
    "json_rdd = df.rdd\n",
    "#df.printSchema()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tweet and user data only from the rdd\n",
    "us_celebrities_df = tweet_and_user_data(json_rdd)\n",
    "us_celebrities_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as parquet\n",
    "us_celebrities_df.write.parquet('../data/american_celebrities/parquet/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FRENCH CELEBRITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load French celebrities tweets into an rdd\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.read.json('../data/french_celebrities/tweets.jsonl')\n",
    "json_rdd = df.rdd\n",
    "#df.printSchema()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tweet and user data only from the rdd\n",
    "fr_celebrities_df = tweet_and_user_data(json_rdd)\n",
    "fr_celebrities_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as parquet\n",
    "fr_celebrities_df.write.parquet('../data/french_celebrities/parquet/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
