{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from datetime import datetime\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from pyspark.sql import SparkSession\n",
    "import re\n",
    "\n",
    "path = '../data/american_politicians/tweets.jsonl'\n",
    "output_name = 'american_politicians_regression_df'\n",
    "workers = 4\n",
    "annotation_threshold = 750\n",
    "resume = False # Resume from last run and take existing regression dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\n",
    "    description=\"Parse tweets JSONL file and create a regression DataFrame\")\n",
    "\n",
    "parser.add_argument('--path', type=str,                     help='Path to the input JSONL file containing tweets')\n",
    "\n",
    "parser.add_argument('--output_name', type=str, default='american_politicians_regression_df',\n",
    "                    help='Name of the output CSV file')\n",
    "parser.add_argument('--workers', type=int, default=4,\n",
    "                    help='Number of worker processes to use for parallel processing')\n",
    "parser.add_argument('--annotation_threshold', type=int, default=750,\n",
    "                    help='Minimum number appearances for an annotation to be considered significant')\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from json file and return RDD\n",
    "def load_data_set(spark, path):\n",
    "\n",
    "    print(\"**Loading data from json file**\")\n",
    "    df = spark.read.json(path)\n",
    "    print(\"Schema written to file\\n\")\n",
    "    print(df.schema, file=open(\"schema\", \"a\"))\n",
    "    return df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_frequent_annotations(rdd,threshold):\n",
    "    print(\"**Getting most frequent annotations** \\n\")\n",
    "    annotations =  rdd.flatMap(lambda x: x['data']).map(lambda x: x['context_annotations']).filter(lambda x: x is not None).flatMap(lambda x: list(set([y['entity']['name'] for y in x]))).map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y).sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "    most_frequent_annotations = annotations.filter(\n",
    "        lambda x: x[1] > threshold).collect()\n",
    "\n",
    "\n",
    "    # remove the first one which is 'Politics' (present in nearly all tweets)\n",
    "    # TODO: check if this is also the case for celebrities\n",
    "    most_frequent_annotations = list(\n",
    "        map(lambda x: x[0], most_frequent_annotations))[1:]\n",
    "\n",
    "    annotation_dict = {annotation: index for index,\n",
    "                      annotation in enumerate(most_frequent_annotations)}\n",
    "    \n",
    "    return most_frequent_annotations ,annotation_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relevant_fields(rdd):\n",
    "    return rdd.filter(lambda x: x['data']).flatMap(lambda x: x['data']).filter(lambda x: x['entities']) \\\n",
    "        .map(lambda x: {\n",
    "            'tweet_text': x['text'],\n",
    "            'tweet_date': x['created_at'],\n",
    "            'tweet_hashtags': x['entities']['hashtags'],\n",
    "            'tweet_mentions': x['entities']['mentions'],\n",
    "            'tweet_urls': x['entities']['urls'],\n",
    "            'user_id': x['author_id'],\n",
    "            'tweet_id': x['id'],\n",
    "            'context_annotations': x['context_annotations'] if x['context_annotations'] else [],\n",
    "            'impression_count': x['public_metrics']['impression_count'],\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_processing_pipeline(json_rdd_data_fields, most_frequent_annotations, annotation_dict, output_name):\n",
    "    \n",
    "    # region INNER FUNCTIONS\n",
    "    def analyse_sentiment(x):\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "        vs = analyzer.polarity_scores(x)\n",
    "        return vs['compound']\n",
    "\n",
    "\n",
    "    def add_key_value(x, key, value):\n",
    "        x[key] = value\n",
    "        return x\n",
    "\n",
    "\n",
    "    def keep_medias_only(x):\n",
    "        urls = x['tweet_urls']\n",
    "        if not urls:\n",
    "            return []\n",
    "        media_urls = [url['media_key']\n",
    "                    for url in urls if 'media_key' in url and url['media_key']]\n",
    "        return media_urls\n",
    "\n",
    "\n",
    "    def get_number_medias(x):\n",
    "        return len(x['tweet_media_keys'])\n",
    "\n",
    "\n",
    "    def get_number_external_urls(x):\n",
    "        return (len(x['tweet_urls']) if x['tweet_urls'] else 0) - x['tweet_medias_count']\n",
    "\n",
    "\n",
    "    def get_period_of_day(x):\n",
    "        hour = datetime.strptime(x['tweet_date'], \"%Y-%m-%dT%H:%M:%S.%fZ\").hour\n",
    "        if hour >= 6 and hour < 12:\n",
    "            return 'morning'\n",
    "        elif hour >= 12 and hour < 18:\n",
    "            return 'afternoon'\n",
    "        else:\n",
    "            return 'night'\n",
    "\n",
    "\n",
    "    def one_hot_encoding(x, encoding_dict):\n",
    "        encoding = [0] * len(encoding_dict)\n",
    "\n",
    "        annotations = x['context_annotations']\n",
    "\n",
    "        if not annotations:\n",
    "            return encoding\n",
    "\n",
    "        for annotation in annotations:\n",
    "            if isinstance(annotation, str):\n",
    "                name = annotation\n",
    "\n",
    "            elif not annotation['entity']:\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                name = annotation['entity']['name']\n",
    "\n",
    "            if name in encoding_dict:\n",
    "                encoding[encoding_dict[name]] = 1\n",
    "\n",
    "        return encoding\n",
    "\n",
    "\n",
    "    def add_dummy_encoding(x, column_names):\n",
    "        encoding = dict(zip(column_names, x['encoded_annotations']))\n",
    "\n",
    "        for key, value in encoding.items():\n",
    "\n",
    "            cleaned = re.sub('[^A-Za-z0-9_]+', '', key.lower())\n",
    "            cleaned = re.sub('__', '_', cleaned)\n",
    "\n",
    "            x[f'dummy_{\"_\".join(cleaned.split(\" \"))}'] = value\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    def add_dummy_tweet_period(x):\n",
    "        time_of_day = x['tweet_period']\n",
    "\n",
    "        x['dummy_tweet_period_morning'] = 0\n",
    "        x['dummy_tweet_period_afternoon'] = 0\n",
    "        x['dummy_tweet_period_night'] = 0\n",
    "\n",
    "        x[f'dummy_tweet_period_{time_of_day}'] = 1\n",
    "\n",
    "        return x\n",
    "\n",
    "    # endregion\n",
    "\n",
    "    # region PROCESSING PIPELINE\n",
    "    # adding sentiment analysis on the tweet text using vader to the data\n",
    "    json_rdd_data_fields = json_rdd_data_fields.map(lambda x : add_key_value(x, 'tweet_sentiment', analyse_sentiment(x['tweet_text'])))\n",
    "\n",
    "    # adding the tweet length to the data\n",
    "    json_rdd_data_fields = json_rdd_data_fields.map(lambda x : add_key_value(x, 'tweet_length', len(x['tweet_text'])))\n",
    "\n",
    "    # adding the number of hashtags to the data\n",
    "    json_rdd_data_fields = json_rdd_data_fields.map(lambda x : add_key_value(x, 'hashtags_count', len(x['tweet_hashtags'])if x['tweet_hashtags'] else 0))\n",
    "\n",
    "    # adding the number of mentions to the data\n",
    "    json_rdd_data_fields = json_rdd_data_fields.map(lambda x : add_key_value(x, 'mentions_count', len(x['tweet_mentions'])if x['tweet_mentions'] else 0))\n",
    "\n",
    "    # adding the media url's only to the data\n",
    "    json_rdd_data_fields = json_rdd_data_fields.map(lambda x : add_key_value(x, 'tweet_media_keys', keep_medias_only(x)))\n",
    "\n",
    "    # adding the number of medias to the data\n",
    "    json_rdd_data_fields = json_rdd_data_fields.map(lambda x : add_key_value(x, 'tweet_medias_count', get_number_medias(x)))\n",
    "\n",
    "    # adding the number of external urls to the data\n",
    "    json_rdd_data_fields = json_rdd_data_fields.map(lambda x : add_key_value(x, 'tweet_external_urls_count', get_number_external_urls(x)))\n",
    "\n",
    "    # adding the period of the day to the data\n",
    "    json_rdd_data_fields = json_rdd_data_fields.map(lambda x : add_key_value(x, 'tweet_period', get_period_of_day(x)))\n",
    "\n",
    "    json_rdd_data_fields = json_rdd_data_fields.map(lambda x : {k: v for k, v in x.items() if k not in ['tweet_mentions', 'tweet_urls', 'tweet_hashtags', 'tweets_media_count']})\n",
    "\n",
    "\n",
    "    # getting the annotations and putting them in clusters using one hot endcoding\n",
    "    json_rdd_data_fields = json_rdd_data_fields.map(lambda x : add_key_value(x, 'encoded_annotations', one_hot_encoding(x, annotation_dict)))\n",
    "\n",
    "\n",
    "    # GENERATE DUMMY VARIABLES FOR CATEGORICAL VARIABLES\n",
    "\n",
    "    # add dummy variables after one hot encoding\n",
    "    json_rdd_data_fields = json_rdd_data_fields.map(lambda x : add_dummy_encoding(x, most_frequent_annotations))\n",
    "\n",
    "    json_rdd_data_fields = json_rdd_data_fields.map(lambda x : add_dummy_tweet_period(x))\n",
    "\n",
    "    # 2. Create a dataframe from the rdd\n",
    "\n",
    "    # endregion\n",
    "    \n",
    "    print(\"** Applying processing pipeline (this may take some time...) **\")\n",
    "    regression_df = json_rdd_data_fields.toDF().drop('context_annotations','encoded_annotations','tweet_date','tweet_media_keys','tweet_period').persist() \n",
    "\n",
    "    regression_df_pd = regression_df.toPandas()\n",
    "    regression_df_pd.to_csv(f'{output_name}.csv', index=False)\n",
    "\n",
    "    print(\"Processed dataframe saved to regression_df.csv \\n\")\n",
    "\n",
    "    return regression_df_pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_pipeline(path, output_name, workers, annotation_threshold):\n",
    "    spark = SparkSession.builder.appName('tweet_loader').master(\n",
    "        f'local[{workers}]').getOrCreate()\n",
    "\n",
    "    print(\"**SparkContext created**\")\n",
    "    print(f'GUI: {spark.sparkContext.uiWebUrl}')\n",
    "    print(f'AppName: {spark.sparkContext.appName}\\n')\n",
    "\n",
    "    rdd = load_data_set(spark, path)\n",
    "\n",
    "    most_frequent_annotations, annotation_dict = get_most_frequent_annotations(rdd, annotation_threshold)\n",
    "\n",
    "    rdd_subset = extract_relevant_fields(rdd)\n",
    "\n",
    "    regression_df = apply_processing_pipeline(rdd_subset, most_frequent_annotations, annotation_dict, output_name)\n",
    "\n",
    "    return regression_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
