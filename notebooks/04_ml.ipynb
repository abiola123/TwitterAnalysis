{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "american_politicians_df = pd.read_parquet('../data/american_politicians/parquet/', engine='pyarrow')\n",
    "# Parse the date column to datetime\n",
    "american_politicians_df['created_at'] = pd.to_datetime(american_politicians_df['created_at'])\n",
    "display(american_politicians_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatterplots(df, columns, x_label):\n",
    "    \"\"\"Plot scatterplots of the given columns against the x_label.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The DataFrame to plot.\n",
    "    columns : list\n",
    "        The columns to plot.\n",
    "    x_label : str\n",
    "        The column to plot on the x-axis.\n",
    "        \"\"\"\n",
    "    nb_rows = len(columns)\n",
    "    height = 4 * nb_rows\n",
    "    fig, axes = plt.subplots(nb_rows, 2, figsize=(16, height))\n",
    "    colors = ['royalblue', 'dodgerblue', 'cornflowerblue', 'skyblue', 'lightsteelblue', 'lightblue', 'lightskyblue', 'powderblue']\n",
    "    i = 0 \n",
    "\n",
    "    for column in columns:\n",
    "        x_label_fmt = x_label.replace('_', ' ').title()\n",
    "        y_label_fmt = column.replace('_', ' ').title()\n",
    "\n",
    "        axes = axes.flatten()\n",
    "        ax1 = axes[i//2*2]\n",
    "        sns.scatterplot(x=df[x_label], y=df[column], color=colors[i//2], alpha=0.5, ax=ax1)\n",
    "        sns.regplot(x=df[x_label], y=df[column], color=colors[i//2], scatter=True, ax=ax1)\n",
    "        ax1.set_xlabel(x_label_fmt, fontsize=8)\n",
    "        ax1.set_ylabel(y_label_fmt, fontsize=8)\n",
    "        ax1.set_title(f'{y_label_fmt} as a Function of {x_label_fmt} in Linear Scale', fontsize=9)\n",
    "        ax1.set_xscale('linear')\n",
    "        ax1.set_yscale('linear')\n",
    "        \n",
    "        ax2 = axes[i//2*2+(i+1)%2]\n",
    "        sns.scatterplot(x=df[x_label], y=df[column], color=colors[i//2], alpha=0.5, ax=ax2)\n",
    "        ax2.set_xlabel(x_label_fmt, fontsize=8)\n",
    "        ax2.set_ylabel(y_label_fmt, fontsize=8)\n",
    "        ax2.set_title(f'{y_label_fmt} as a Function of {x_label_fmt} in Log Scale', fontsize=9)\n",
    "        ax2.set_xscale('log')\n",
    "        ax2.set_yscale('log')\n",
    "        \n",
    "        i += 2\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the scatterplots of the engagement counts against impression count\n",
    "columns = ['retweet_count', 'reply_count', 'like_count', 'quote_count']\n",
    "# plot_scatterplots(american_politicians_df, columns, 'impression_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the scatterplots of the impression counts against follower counts\n",
    "columns = ['impression_count']\n",
    "# plot_scatterplots(american_politicians_df, columns, 'followers_count')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET FEATURES FROM DATAFRAME"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweet info:\n",
    "- Tweet length  âœ…\n",
    "- Time of the tweet (morning, afternoon, night) âœ…\n",
    "- Sentiment of the tweet (score computed by model) âœ…\n",
    "- Number of hashtags âœ…\n",
    "- Number of mentions âœ…\n",
    "- Number of url's âœ…\n",
    "- Media type (video, image, text,..) => attention c un tableau\n",
    "- (To clarify: location) ðŸš§\n",
    "\n",
    "User info:\n",
    "- Verified âœ…\n",
    "- Profile creation date âœ…\n",
    "- (To clarify: Tweet frequency) ðŸš§"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRACTING FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Import spark and open json file\n",
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"20g\").getOrCreate()\n",
    "df = spark.read.json('../data/american_politicians/tweets.jsonl')\n",
    "json_rdd = df.rdd\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_rdd.flatMap(lambda x: x['data']).map(lambda x: {'text':x['text'],'context_annotations':x['context_annotations'], 'inner_annotations':x['entities']['annotations']}).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get total number of tweets\n",
    "json_rdd.flatMap(lambda x: x['data']).count()\n",
    "\n",
    "#get total number of tweets without context annotations\n",
    "json_rdd.flatMap(lambda x: x['data']).filter(lambda x: not x['context_annotations']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most common context annotations that will be used for clustering later \n",
    "\n",
    "annotations = json_rdd.flatMap(lambda x: x['data']).map(lambda x: x['context_annotations']).filter(lambda x: x is not None).flatMap(lambda x: list(set([y['entity']['name'] for y in x]))).map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y).sortBy(lambda x: x[1], ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_annotations = annotations.filter(lambda x: x[1] > 750).collect()\n",
    "# remove the first one which is 'Politics' (present in nearly all tweets)\n",
    "most_frequent_annotations = list(\n",
    "    map(lambda x: x[0], most_frequent_annotations))[1:]\n",
    "\n",
    "annotation_dict = {annotation: index for index,\n",
    "                   annotation in enumerate(most_frequent_annotations)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "zero_shot_classifier1 = pipeline('zero-shot-classification',\n",
    "                                model='facebook/bart-large-mnli')\n",
    "\n",
    "zero_shot_classifier2 = pipeline('zero-shot-classification', model='roberta-large-mnli')\n",
    "\n",
    "zero_shot_classifier3 = pipeline(\n",
    "    'zero-shot-classification', model='huggingface/distilbert-base-uncased-finetuned-mnli')\n",
    "\n",
    "zero_shot_classifier4 = pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-3')\n",
    "\n",
    "zero_shot_classifier5 = pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-9')\n",
    "\n",
    "\n",
    "#broadcast_classifier = spark.sparkContext.broadcast(zero_shot_classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Joe Biden is the 46th President of the United States, having taken office on January 20, 2021. He served as Vice President under President Barack Obama from 2009 to 2017. His presidency has focused on a range of issues, including COVID-19 pandemic response, climate change, immigration reform, and economic recovery.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "zero_shot_classifier1(sentence, most_frequent_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "zero_shot_classifier2(sentence, most_frequent_annotations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "zero_shot_classifier4(sentence, most_frequent_annotations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "zero_shot_classifier5(sentence, most_frequent_annotations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# Function to get the sentiment of a tweet\n",
    "def analyse_sentiment(x):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    vs = analyzer.polarity_scores(x)\n",
    "    return vs['compound']\n",
    "\n",
    "def add_key_value(x, key, value):\n",
    "    x[key] = value\n",
    "    return x\n",
    "\n",
    "def keep_medias_only(x):\n",
    "    urls = x['tweet_urls']\n",
    "    if not urls:\n",
    "        return []\n",
    "    media_urls = [url['media_key'] for url in urls if 'media_key' in url and url['media_key']]\n",
    "    return media_urls\n",
    "\n",
    "def get_number_medias(x):\n",
    "    return len(x['tweet_media_keys'])\n",
    "\n",
    "def get_number_external_urls(x):\n",
    "    return (len(x['tweet_urls']) if x['tweet_urls'] else 0) - x['tweet_medias_count']\n",
    "\n",
    "def get_period_of_day(x):\n",
    "    hour = datetime.strptime(x['tweet_date'],\"%Y-%m-%dT%H:%M:%S.%fZ\").hour\n",
    "    if hour >= 6 and hour < 12:\n",
    "        return 'morning'\n",
    "    elif hour >= 12 and hour < 18:\n",
    "        return 'afternoon'\n",
    "    else:\n",
    "        return 'night'\n",
    "\n",
    "def one_hot_encoding(x, encoding_dict):\n",
    "    encoding = [0] * len(encoding_dict)\n",
    "\n",
    "    annotations = x['context_annotations']\n",
    "\n",
    "    if not annotations:\n",
    "        return encoding\n",
    "\n",
    "    for annotation in annotations:\n",
    "        if isinstance(annotation,str):\n",
    "            name = annotation\n",
    "        \n",
    "        elif not annotation['entity']:\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            name = annotation['entity']['name']\n",
    "\n",
    "        if name in encoding_dict:\n",
    "           encoding[encoding_dict[name]] = 1\n",
    "\n",
    "    return encoding\n",
    "\n",
    "def add_dummy_encoding(x, column_names):\n",
    "    encoding = dict(zip(column_names, x['encoded_annotations']))\n",
    "\n",
    "    for key, value in encoding.items():\n",
    "\n",
    "        cleaned = re.sub('[^A-Za-z0-9_]+', '', key.lower())\n",
    "        cleaned = re.sub('__', '_', cleaned)\n",
    "\n",
    "        x[f'dummy_{\"_\".join(cleaned.split(\" \"))}'] = value\n",
    "\n",
    "    return x\n",
    "\n",
    "def add_dummy_tweet_period(x):\n",
    "    time_of_day = x['tweet_period']\n",
    "\n",
    "    x['dummy_tweet_period_morning'] = 0\n",
    "    x['dummy_tweet_period_afternoon'] = 0\n",
    "    x['dummy_tweet_period_night'] = 0\n",
    "\n",
    "    x[f'dummy_tweet_period_{time_of_day}'] = 1\n",
    "\n",
    "    return x\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_rdd_data_fields =json_rdd.filter(lambda x: x['data']).flatMap(lambda x: x['data']).filter(lambda x: x['entities']) \\\n",
    ".map(lambda x : {\n",
    "    'tweet_text': x['text'],\n",
    "    'tweet_date': x['created_at'],\n",
    "    'tweet_hashtags': x['entities']['hashtags'],\n",
    "    'tweet_mentions': x['entities']['mentions'],\n",
    "    'tweet_urls': x['entities']['urls'], \n",
    "    'user_id': x['author_id'],\n",
    "    'tweet_id': x['id'],\n",
    "    'context_annotations': x['context_annotations'] if x['context_annotations'] else [],\n",
    "    'impression_count': x['public_metrics']['impression_count'],\n",
    "    # 'retweet_count': x['public_metrics']['retweet_count'],\n",
    "    # 'reply_count': x['public_metrics']['reply_count'],\n",
    "    # 'like_count': x['public_metrics']['like_count'],\n",
    "})\n",
    "\n",
    "# adding sentiment analysis on the tweet text using vader to the data\n",
    "json_rdd_data_fields = json_rdd_data_fields.map(lambda x : add_key_value(x, 'tweet_sentiment', analyse_sentiment(x['tweet_text'])))\n",
    "\n",
    "# adding the tweet length to the data\n",
    "json_rdd_data_fields = json_rdd_data_fields.map(lambda x : add_key_value(x, 'tweet_length', len(x['tweet_text'])))\n",
    "\n",
    "# adding the number of hashtags to the data\n",
    "json_rdd_data_fields = json_rdd_data_fields.map(lambda x : add_key_value(x, 'hashtags_count', len(x['tweet_hashtags'])if x['tweet_hashtags'] else 0))\n",
    "\n",
    "# adding the number of mentions to the data\n",
    "json_rdd_data_fields = json_rdd_data_fields.map(lambda x : add_key_value(x, 'mentions_count', len(x['tweet_mentions'])if x['tweet_mentions'] else 0))\n",
    "\n",
    "# adding the media url's only to the data\n",
    "json_rdd_data_fields = json_rdd_data_fields.map(lambda x : add_key_value(x, 'tweet_media_keys', keep_medias_only(x)))\n",
    "\n",
    "# adding the number of medias to the data\n",
    "json_rdd_data_fields = json_rdd_data_fields.map(lambda x : add_key_value(x, 'tweet_medias_count', get_number_medias(x)))\n",
    "\n",
    "# adding the number of external urls to the data\n",
    "json_rdd_data_fields = json_rdd_data_fields.map(lambda x : add_key_value(x, 'tweet_external_urls_count', get_number_external_urls(x)))\n",
    "\n",
    "# adding the period of the day to the data\n",
    "json_rdd_data_fields = json_rdd_data_fields.map(lambda x : add_key_value(x, 'tweet_period', get_period_of_day(x)))\n",
    "\n",
    "json_rdd_data_fields = json_rdd_data_fields.map(lambda x : {k: v for k, v in x.items() if k not in ['tweet_mentions', 'tweet_urls', 'tweet_hashtags', 'tweets_media_count']})\n",
    "\n",
    "\n",
    "# getting the annotations and putting them in clusters using one hot endcoding\n",
    "json_rdd_data_fields = json_rdd_data_fields.map(lambda x : add_key_value(x, 'encoded_annotations', one_hot_encoding(x, annotation_dict)))\n",
    "\n",
    "\n",
    "# GENERATE DUMMY VARIABLES FOR CATEGORICAL VARIABLES\n",
    "\n",
    "# add dummy variables after one hot encoding\n",
    "json_rdd_data_fields = json_rdd_data_fields.map(lambda x : add_dummy_encoding(x, most_frequent_annotations))\n",
    "\n",
    "json_rdd_data_fields = json_rdd_data_fields.map(lambda x : add_dummy_tweet_period(x))\n",
    "\n",
    "# 2. Create a dataframe from the rdd\n",
    "\n",
    "# transforming the data to a dataframe\n",
    "regression_df = json_rdd_data_fields.toDF().drop('context_annotations','encoded_annotations','tweet_date','tweet_media_keys','tweet_period').persist()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_df_pd = regression_df.toPandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_df_pd.to_csv('regression_df.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_regressor_columns_string(columns):\n",
    "  regressor_columns = list(filter(lambda x: x != 'impression_count',columns))\n",
    "  regressor_columns_string = \"+\".join(regressor_columns)\n",
    "  return regressor_columns_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ols regression follower count on the other variables\n",
    "import statsmodels.formula.api as smf\n",
    "regressor_columns_string = create_regressor_columns_string(regression_df_pd.columns)\n",
    "mod = smf.ols(formula=f'impression_count ~ {regressor_columns_string}', data=regression_df_pd)\n",
    "res = mod.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the variables that are not significant\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "\n",
    "print(regression_df_pd.columns)\n",
    "\n",
    "regression_df_pd['impression_count'] = np.log(1+ regression_df_pd['impression_count'])\n",
    "\n",
    "display(regression_df_pd)\n",
    "\n",
    "regressor_columns = list(filter(lambda x: x not in [\n",
    "                         'tweet_external_urls_count', 'dummy_tweet_period_night', 'dummy_joemanchin', 'dummy_chrismurphy', 'dummy_financialservicesbusiness', 'dummy_inflationintheunitedstates', 'dummy_joebiden', 'dummy_politicalfigures', 'dummy_northcarolina', 'dummy_tedcruz', 'dummy_sportsfitnessbusiness', 'dummy_unitedstatescongress', 'dummy_markwarner', 'dummy_politicalnews', 'dummy_chriscoons'], regression_df_pd.columns))\n",
    "\n",
    "regressor_columns_string = create_regressor_columns_string(\n",
    "    regressor_columns)\n",
    "mod = smf.ols(\n",
    "    formula=f'impression_count ~ {regressor_columns_string}', data=regression_df_pd)\n",
    "res = mod.fit()\n",
    "print(res.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_rdd_includes_fields =json_rdd \\\n",
    ".map(lambda x : {\n",
    "    'user_profile_creation_date': x['includes']['users'][0]['created_at'],\n",
    "    'user_verified': x['includes']['users'][0]['verified'],\n",
    "    'user_creation_date': x['includes']['users'][0]['created_at'],\n",
    "    'user_id': x['includes']['users'][0]['id'],\n",
    "})\n",
    "\n",
    "json_inclued_fields_df = json_rdd_includes_fields.toDF(['user_profile_creation_date', 'user_verified', 'user_creation_date', 'user_id'])\n",
    "\n",
    "json_inclued_fields_df.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
